{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 17GB\n",
      "Dimensions:     (valid_time: 365, latitude: 721, longitude: 1440)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 2002-01-01T12:00:00 ... 2002-...\n",
      "  * latitude    (latitude) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * longitude   (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 6kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    mwd         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    mwp         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    swh         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    u10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    d2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    msl         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sst         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-07-05T14:27 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['CDS_Data/data_stream-wave_stepType-instant.nc', 'CDS_Data/data_stream-oper_stepType-instant.nc', \n",
    "           'CDS_Data/data_stream-oper_stepType-accum.nc']\n",
    "\n",
    "# Create an empty list to store the Datasets\n",
    "datasets = []\n",
    "\n",
    "# Open each NetCDF file and append its Dataset to the list\n",
    "for fp in file_paths:\n",
    "    ds = xr.open_dataset(fp)\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Merge all Datasets in the list into a single Dataset\n",
    "combined_dataset = xr.merge(datasets)\n",
    "\n",
    "# Inspect your dataset\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Latitude Coordinates ---\n",
      "<xarray.DataArray 'latitude' (latitude: 721)> Size: 6kB\n",
      "array([-90.  , -89.75, -89.5 , ...,  89.5 ,  89.75,  90.  ])\n",
      "Coordinates:\n",
      "  * latitude  (latitude) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "    number    int64 8B 0\n",
      "Attributes:\n",
      "    units:             degrees_north\n",
      "    standard_name:     latitude\n",
      "    long_name:         latitude\n",
      "    stored_direction:  decreasing\n",
      "Minimum Latitude: -90.0\n",
      "Maximum Latitude: 90.0\n",
      "\n",
      "--- Checking Longitude Coordinates ---\n",
      "<xarray.DataArray 'longitude' (longitude: 1440)> Size: 12kB\n",
      "array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n",
      "       3.5975e+02])\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "    number     int64 8B 0\n",
      "Attributes:\n",
      "    units:          degrees_east\n",
      "    standard_name:  longitude\n",
      "    long_name:      longitude\n",
      "Minimum Longitude: 0.0\n",
      "Maximum Longitude: 359.75\n",
      "\n",
      "--- Full Dataset Info ---\n",
      "<xarray.Dataset> Size: 17GB\n",
      "Dimensions:     (valid_time: 365, latitude: 721, longitude: 1440)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 2002-01-01T12:00:00 ... 2002-...\n",
      "  * latitude    (latitude) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * longitude   (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 6kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    mwd         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    mwp         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    swh         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    u10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    d2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    msl         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sst         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-07-05T14:27 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Checking Latitude Coordinates ---\")\n",
    "print(combined_dataset['latitude'])\n",
    "print(f\"Minimum Latitude: {combined_dataset['latitude'].min().item()}\")\n",
    "print(f\"Maximum Latitude: {combined_dataset['latitude'].max().item()}\")\n",
    "\n",
    "print(\"\\n--- Checking Longitude Coordinates ---\")\n",
    "print(combined_dataset['longitude'])\n",
    "print(f\"Minimum Longitude: {combined_dataset['longitude'].min().item()}\")\n",
    "print(f\"Maximum Longitude: {combined_dataset['longitude'].max().item()}\")\n",
    "\n",
    "print(\"\\n--- Full Dataset Info ---\")\n",
    "print(combined_dataset) # Look closely at the dimensions here too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 40MB\n",
      "Dimensions:     (valid_time: 365, latitude: 41, longitude: 61)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 2002-01-01T12:00:00 ... 2002-...\n",
      "  * latitude    (latitude) float64 328B 45.0 45.25 45.5 ... 54.5 54.75 55.0\n",
      "  * longitude   (longitude) float64 488B 5.0 5.25 5.5 5.75 ... 19.5 19.75 20.0\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 6kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    mwd         (valid_time, latitude, longitude) float32 4MB nan nan ... 344.9\n",
      "    mwp         (valid_time, latitude, longitude) float32 4MB nan nan ... 6.452\n",
      "    swh         (valid_time, latitude, longitude) float32 4MB nan nan ... 2.092\n",
      "    u10         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    d2m         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    msl         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    sst         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 4MB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 4MB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-07-05T14:27 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "north_bound_lat_chch = 55.0  # Remember: -43.0 is \"more north\" than -44.0\n",
    "south_bound_lat_chch = 45.0\n",
    "west_bound_lon_chch = 5.0\n",
    "east_bound_lon_chch = 20.0\n",
    "\n",
    "# 1. Select the data for the specified Christchurch region\n",
    "# IMPORTANT: For 'latitude' which is 'decreasing' (90 to -90), the slice order is (higher_value, lower_value)\n",
    "ds = combined_dataset.sel(\n",
    "    latitude=slice(south_bound_lat_chch, north_bound_lat_chch), # Correct order for decreasing latitude\n",
    "    longitude=slice(west_bound_lon_chch, east_bound_lon_chch)   # Standard order for increasing longitude (0 to 359.5)\n",
    ")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        number expver         mwd       mwp  \\\n",
      "valid_time          latitude longitude                                        \n",
      "2002-01-01 12:00:00 45.0     5.00            0   0001         NaN       NaN   \n",
      "                             5.25            0   0001         NaN       NaN   \n",
      "                             5.50            0   0001         NaN       NaN   \n",
      "                             5.75            0   0001         NaN       NaN   \n",
      "                             6.00            0   0001         NaN       NaN   \n",
      "...                                        ...    ...         ...       ...   \n",
      "2002-12-31 12:00:00 55.0     19.00           0   0001  351.784027  6.344384   \n",
      "                             19.25           0   0001         NaN       NaN   \n",
      "                             19.50           0   0001  347.205902  6.363671   \n",
      "                             19.75           0   0001         NaN       NaN   \n",
      "                             20.00           0   0001  344.869965  6.451562   \n",
      "\n",
      "                                             swh       u10       v10  \\\n",
      "valid_time          latitude longitude                                 \n",
      "2002-01-01 12:00:00 45.0     5.00            NaN -0.804764 -1.102310   \n",
      "                             5.25            NaN -0.738358 -0.229263   \n",
      "                             5.50            NaN -0.763748  0.397690   \n",
      "                             5.75            NaN -1.006912  0.909409   \n",
      "                             6.00            NaN -1.389725  0.598862   \n",
      "...                                          ...       ...       ...   \n",
      "2002-12-31 12:00:00 55.0     19.00      2.196946  5.939423 -8.245361   \n",
      "                             19.25           NaN  6.187469 -8.601807   \n",
      "                             19.50      2.306565  6.205048 -8.883057   \n",
      "                             19.75           NaN  5.865204 -8.530518   \n",
      "                             20.00      2.091721  4.832001 -6.674072   \n",
      "\n",
      "                                               d2m         t2m          msl  \\\n",
      "valid_time          latitude longitude                                        \n",
      "2002-01-01 12:00:00 45.0     5.00       267.758881  274.061951  103693.1250   \n",
      "                             5.25       266.268646  273.729919  103658.1250   \n",
      "                             5.50       265.891693  271.888123  103644.6250   \n",
      "                             5.75       261.844818  267.663513  103620.3750   \n",
      "                             6.00       256.305756  261.890076  103619.1250   \n",
      "...                                            ...         ...          ...   \n",
      "2002-12-31 12:00:00 55.0     19.00      265.124939  271.457428  101757.9375   \n",
      "                             19.25      265.246033  271.527740  101729.1875   \n",
      "                             19.50      265.076111  271.633209  101700.4375   \n",
      "                             19.75      264.966736  271.465240  101673.6875   \n",
      "                             20.00      265.292908  270.521881  101650.9375   \n",
      "\n",
      "                                               sst             sp        tp  \n",
      "valid_time          latitude longitude                                       \n",
      "2002-01-01 12:00:00 45.0     5.00              NaN   99381.828125  0.000000  \n",
      "                             5.25              NaN   96759.828125  0.000000  \n",
      "                             5.50              NaN   93320.828125  0.000000  \n",
      "                             5.75              NaN   88884.828125  0.000000  \n",
      "                             6.00              NaN   84657.828125  0.000000  \n",
      "...                                            ...            ...       ...  \n",
      "2002-12-31 12:00:00 55.0     19.00      276.916260  101738.914062  0.000000  \n",
      "                             19.25      276.927002  101705.914062  0.000002  \n",
      "                             19.50      276.882080  101663.914062  0.000012  \n",
      "                             19.75      276.850830  101622.914062  0.000017  \n",
      "                             20.00             NaN  101561.914062  0.000015  \n",
      "\n",
      "[912865 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "df = ds.to_dataframe()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addressing NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN counts per column in DataFrame:\n",
      "number         0\n",
      "expver         0\n",
      "mwd       884395\n",
      "mwp       884395\n",
      "swh       884395\n",
      "u10            0\n",
      "v10            0\n",
      "d2m            0\n",
      "t2m            0\n",
      "msl            0\n",
      "sst       821250\n",
      "sp             0\n",
      "tp             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNaN counts per column in DataFrame:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number    0\n",
      "expver    0\n",
      "u10       0\n",
      "v10       0\n",
      "d2m       0\n",
      "t2m       0\n",
      "msl       0\n",
      "sp        0\n",
      "tp        0\n",
      "dtype: int64\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['mwd', 'mwp', 'swh', 'sst']\n",
    "\n",
    "# Drop the columns\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(print(df_cleaned.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after sorting and re-indexing (first few rows):\n",
      "                                        number expver       u10       v10  \\\n",
      "valid_time          latitude longitude                                      \n",
      "2002-01-01 12:00:00 45.0     5.00            0   0001 -0.804764 -1.102310   \n",
      "                             5.25            0   0001 -0.738358 -0.229263   \n",
      "                             5.50            0   0001 -0.763748  0.397690   \n",
      "                             5.75            0   0001 -1.006912  0.909409   \n",
      "                             6.00            0   0001 -1.389725  0.598862   \n",
      "\n",
      "                                               d2m         t2m         msl  \\\n",
      "valid_time          latitude longitude                                       \n",
      "2002-01-01 12:00:00 45.0     5.00       267.758881  274.061951  103693.125   \n",
      "                             5.25       266.268646  273.729919  103658.125   \n",
      "                             5.50       265.891693  271.888123  103644.625   \n",
      "                             5.75       261.844818  267.663513  103620.375   \n",
      "                             6.00       256.305756  261.890076  103619.125   \n",
      "\n",
      "                                                  sp   tp  \n",
      "valid_time          latitude longitude                     \n",
      "2002-01-01 12:00:00 45.0     5.00       99381.828125  0.0  \n",
      "                             5.25       96759.828125  0.0  \n",
      "                             5.50       93320.828125  0.0  \n",
      "                             5.75       88884.828125  0.0  \n",
      "                             6.00       84657.828125  0.0  \n",
      "Original shape: (912865, 9)\n"
     ]
    }
   ],
   "source": [
    "df_reset = df_cleaned.reset_index()\n",
    "\n",
    "# Convert 'valid_time' to datetime\n",
    "df_reset['valid_time'] = pd.to_datetime(df_reset['valid_time'])\n",
    "\n",
    "# Sort by time, then latitude, then longitude to ensure consistent grid formation\n",
    "df_sorted = df_reset.sort_values(by=['valid_time', 'latitude', 'longitude'])\n",
    "\n",
    "# Re-set the MultiIndex for easier handling, though we might iterate through it\n",
    "df_final = df_sorted.set_index(['valid_time', 'latitude', 'longitude'])\n",
    "\n",
    "print(\"DataFrame after sorting and re-indexing (first few rows):\")\n",
    "print(df_final.head())\n",
    "print(f\"Original shape: {df_final.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid dimensions: Latitude=41, Longitude=61\n",
      "Number of features (variables): 9\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Determine Grid Dimensions ---\n",
    "# Get unique latitudes and longitudes\n",
    "latitudes = df_final.index.get_level_values('latitude').unique().sort_values()\n",
    "longitudes = df_final.index.get_level_values('longitude').unique().sort_values()\n",
    "\n",
    "num_lat = len(latitudes)\n",
    "num_lon = len(longitudes)\n",
    "num_features = len(df_final.columns) # All columns are features except if we designate one as target later\n",
    "\n",
    "print(f\"\\nGrid dimensions: Latitude={num_lat}, Longitude={num_lon}\")\n",
    "print(f\"Number of features (variables): {num_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of gridded_data: (365, 41, 61, 9)\n",
      "NaNs in gridded data after initial reshape: 0\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Reshape to Gridded Data (Time, Lat, Lon, Features) ---\n",
    "# Get all unique timestamps\n",
    "all_times = df_final.index.get_level_values('valid_time').unique().sort_values()\n",
    "num_times = len(all_times)\n",
    "\n",
    "# Create an empty array to store the gridded data\n",
    "# Shape: (num_times, num_lat, num_lon, num_features)\n",
    "gridded_data = np.full((num_times, num_lat, num_lon, num_features), np.nan) # Use nan for missing spots\n",
    "\n",
    "# Map latitude and longitude to array indices\n",
    "lat_to_idx = {lat: i for i, lat in enumerate(latitudes)}\n",
    "lon_to_idx = {lon: i for i, lon in enumerate(longitudes)}\n",
    "\n",
    "# Populate the gridded_data array\n",
    "for t_idx, time in enumerate(all_times):\n",
    "    # Select data for the current timestamp\n",
    "    time_slice_df = df_final.loc[time]\n",
    "\n",
    "    for (lat, lon), row_data in time_slice_df.iterrows():\n",
    "        lat_idx = lat_to_idx[lat]\n",
    "        lon_idx = lon_to_idx[lon]\n",
    "        gridded_data[t_idx, lat_idx, lon_idx, :] = row_data.values\n",
    "\n",
    "print(f\"\\nShape of gridded_data: {gridded_data.shape}\")\n",
    "# Check if there are any NaNs left from this reshaping.\n",
    "# There shouldn't be if all combinations of lat/lon exist for each time.\n",
    "print(f\"NaNs in gridded data after initial reshape: {np.sum(np.isnan(gridded_data))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define Input (X) and Target (y) Variables ---\n",
    "# 'tp' is total precipitation, which we want to predict.\n",
    "# Let's find its column index.\n",
    "feature_names = df_final.columns.tolist()\n",
    "tp_col_idx = feature_names.index('tp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_sequences (input): (360, 5, 41, 61, 8)\n",
      "Shape of y_sequences (target): (360, 1, 41, 61)\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Create Sequences for ConvLSTM ---\n",
    "# We need to define:\n",
    "# - `n_steps_in`: how many past time steps to use as input\n",
    "# - `n_steps_out`: how many future time steps to predict (for rainfall, often 1)\n",
    "\n",
    "n_steps_in = 5  # Example: Use the past 5 time steps (e.g., 5 days if data is daily)\n",
    "n_steps_out = 1 # Example: Predict the next 1 time step (e.g., next day's rainfall)\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out, tp_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        # Input sequence (all features except 'tp' for n_steps_in)\n",
    "        # We need to exclude 'tp' from the input features\n",
    "        X_seq_features = np.delete(data[i:(i + n_steps_in)], tp_idx, axis=-1)\n",
    "        X.append(X_seq_features)\n",
    "\n",
    "        # Output sequence (only 'tp' for n_steps_out)\n",
    "        y_seq_tp = data[(i + n_steps_in):(i + n_steps_in + n_steps_out), :, :, tp_idx]\n",
    "        y.append(y_seq_tp)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(gridded_data, n_steps_in, n_steps_out, tp_col_idx)\n",
    "\n",
    "print(f\"\\nShape of X_sequences (input): {X_sequences.shape}\")\n",
    "print(f\"Shape of y_sequences (target): {y_sequences.shape}\")\n",
    "\n",
    "# X_sequences shape should be (num_samples, n_steps_in, num_lat, num_lon, num_features - 1)\n",
    "# y_sequences shape should be (num_samples, n_steps_out, num_lat, num_lon) - if predicting one variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 288 sequences\n",
      "Test set size: 72 sequences\n",
      "X_train shape: (288, 5, 41, 61, 8)\n",
      "y_train shape: (288, 1, 41, 61)\n",
      "X_test shape: (72, 5, 41, 61, 8)\n",
      "y_test shape: (72, 1, 41, 61)\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Time-based Train-Test Split ---\n",
    "# The data covers 2002. Let's use the first ~9 months for training and the last ~3 months for testing.\n",
    "# A common split is 80% train / 20% test.\n",
    "# Since X_sequences and y_sequences are already ordered by time, we can simply split the arrays.\n",
    "\n",
    "# Determine the split point based on the number of sequences\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X_sequences) * split_ratio)\n",
    "\n",
    "X_train = X_sequences[:split_index]\n",
    "y_train = y_sequences[:split_index]\n",
    "X_test = X_sequences[split_index:]\n",
    "y_test = y_sequences[split_index:]\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)} sequences\")\n",
    "print(f\"Test set size: {len(X_test)} sequences\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (channels) in X_sequences before normalization: ['number', 'expver', 'u10', 'v10', 'd2m', 't2m', 'msl', 'sp']\n",
      "Number of features (channels) in X_sequences: 8\n"
     ]
    }
   ],
   "source": [
    "original_feature_names_in_X_sequences = [col for col in df_final.columns if col != 'tp']\n",
    "\n",
    "\n",
    "print(f\"Features (channels) in X_sequences before normalization: {original_feature_names_in_X_sequences}\")\n",
    "print(f\"Number of features (channels) in X_sequences: {len(original_feature_names_in_X_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Feature 'number' is constant and will not be effectively normalized by RobustScaler.\n",
      "It's generally recommended to remove constant features before creating sequences if they are truly constant.\n",
      "Warning: Feature 'expver' is constant and will not be effectively normalized by RobustScaler.\n",
      "It's generally recommended to remove constant features before creating sequences if they are truly constant.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Identify and Exclude Constant Features (if any) ---\n",
    "# This step should ideally be done BEFORE creating X_sequences.\n",
    "# Let's check for constants in the original `df_final` used to create the gridded data.\n",
    "# If df_final is not available, you would need to load a sample or assume.\n",
    "\n",
    "constant_features_found = []\n",
    "if 'df_final' in locals():\n",
    "    # Check only features that ended up in X_sequences\n",
    "    for col in original_feature_names_in_X_sequences:\n",
    "        if df_final[col].nunique() == 1:\n",
    "            constant_features_found.append(col)\n",
    "            print(f\"Warning: Feature '{col}' is constant and will not be effectively normalized by RobustScaler.\")\n",
    "            print(f\"It's generally recommended to remove constant features before creating sequences if they are truly constant.\")\n",
    "else:\n",
    "    print(\"Warning: `df_final` not found. Cannot verify if features are constant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train reshaped for scaler fitting: (3601440, 8)\n",
      "\n",
      "RobustScaler for features (X) fitted on X_train data.\n",
      "X_train normalized reshaped shape: (3601440, 8)\n",
      "X_test normalized reshaped shape: (900360, 8)\n",
      "\n",
      "Final X_train_normalized shape (ready for ConvLSTM input): (288, 5, 41, 61, 8)\n",
      "Final X_test_normalized shape (ready for ConvLSTM input): (72, 5, 41, 61, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Normalization of X_train and X_test features ---\n",
    "\n",
    "# Get the shape of X_train to reshape back later\n",
    "shape_x_train = X_train.shape # (num_samples, n_steps_in, num_lat, num_lon, num_features_for_scaling)\n",
    "num_features_for_scaling = shape_x_train[-1] # Number of channels (e.g., 8)\n",
    "\n",
    "# Flatten X_train for scaler fitting: (total_elements, num_features)\n",
    "# This combines all samples, timesteps, lat, and lon into a single dimension for feature scaling\n",
    "X_train_reshaped_for_scaler = X_train.reshape(-1, num_features_for_scaling)\n",
    "print(f\"\\nX_train reshaped for scaler fitting: {X_train_reshaped_for_scaler.shape}\")\n",
    "\n",
    "# Initialize RobustScaler for features\n",
    "scaler_X = RobustScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data's reshaped features\n",
    "scaler_X.fit(X_train_reshaped_for_scaler)\n",
    "print(\"\\nRobustScaler for features (X) fitted on X_train data.\")\n",
    "\n",
    "# Transform both training and test data using the fitted scaler\n",
    "X_train_normalized_reshaped = scaler_X.transform(X_train_reshaped_for_scaler)\n",
    "X_test_normalized_reshaped = scaler_X.transform(X_test.reshape(-1, num_features_for_scaling)) # Reshape X_test on the fly\n",
    "\n",
    "print(f\"X_train normalized reshaped shape: {X_train_normalized_reshaped.shape}\")\n",
    "print(f\"X_test normalized reshaped shape: {X_test_normalized_reshaped.shape}\")\n",
    "\n",
    "# Reshape the normalized data back to the ConvLSTM input format (5D)\n",
    "X_train_normalized = X_train_normalized_reshaped.reshape(shape_x_train)\n",
    "X_test_normalized = X_test_normalized_reshaped.reshape(X_test.shape) # Use X_test's original shape\n",
    "\n",
    "print(f\"\\nFinal X_train_normalized shape (ready for ConvLSTM input): {X_train_normalized.shape}\")\n",
    "print(f\"Final X_test_normalized shape (ready for ConvLSTM input): {X_test_normalized.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train reshaped for scaler fitting: (720288, 1)\n",
      "RobustScaler for target (y/tp) fitted on y_train data.\n",
      "Final y_train_normalized shape (ready for ConvLSTM output): (288, 1, 41, 61)\n",
      "Final y_test_normalized shape (ready for ConvLSTM output): (72, 1, 41, 61)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Normalization for the Target Variable (y - Total Precipitation 'tp') ---\n",
    "# 'tp' values are non-negative and often have a high concentration at zero, with some large values.\n",
    "# RobustScaler can work, but consider alternatives like log1p if zeros are dominant and you want to reduce skewness.\n",
    "\n",
    "# Get the shape of y_train to reshape back later\n",
    "shape_y_train = y_train.shape # (num_samples, n_steps_out, num_lat, num_lon)\n",
    "num_features_y = 1 # 'tp' is a single target variable\n",
    "\n",
    "# Flatten y_train for scaler fitting: (total_elements, 1)\n",
    "y_train_reshaped_for_scaler = y_train.reshape(-1, num_features_y)\n",
    "print(f\"\\ny_train reshaped for scaler fitting: {y_train_reshaped_for_scaler.shape}\")\n",
    "\n",
    "# Initialize RobustScaler for the target 'tp'\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data's reshaped target\n",
    "scaler_y.fit(y_train_reshaped_for_scaler)\n",
    "print(\"RobustScaler for target (y/tp) fitted on y_train data.\")\n",
    "\n",
    "# Transform both training and test target data\n",
    "y_train_normalized_reshaped = scaler_y.transform(y_train_reshaped_for_scaler)\n",
    "y_test_normalized_reshaped = scaler_y.transform(y_test.reshape(-1, num_features_y))\n",
    "\n",
    "y_train_normalized = y_train_normalized_reshaped.reshape(shape_y_train)\n",
    "y_test_normalized = y_test_normalized_reshaped.reshape(y_test.shape)\n",
    "\n",
    "print(f\"Final y_train_normalized shape (ready for ConvLSTM output): {y_train_normalized.shape}\")\n",
    "print(f\"Final y_test_normalized shape (ready for ConvLSTM output): {y_test_normalized.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. Training on CPU.\n",
      "\n",
      "Model Input Parameters:\n",
      "  n_steps_in (sequence length): 5\n",
      "  num_lat (grid rows): 41\n",
      "  num_lon (grid columns): 61\n",
      "  num_channels (input features): 8\n",
      "  n_steps_out (prediction horizon): 1\n",
      "Target 'y' adjusted from (288, 1, 41, 61) to (288, 41, 61) for n_steps_out=1.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv_lstm2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">461,056</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_lstm2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ConvLSTM2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">41</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv_lstm2d_6 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │       \u001b[38;5;34m461,056\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv_lstm2d_7 (\u001b[38;5;33mConvLSTM2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │            \u001b[38;5;34m65\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m41\u001b[0m, \u001b[38;5;34m61\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">756,801</span> (2.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m756,801\u001b[0m (2.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">756,545</span> (2.89 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m756,545\u001b[0m (2.89 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> (1.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m256\u001b[0m (1.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting model training...\n",
      "Epoch 1/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 27.1708 - mae: 2.2649\n",
      "Epoch 1: val_loss improved from inf to 16.72049, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 319ms/step - loss: 27.0930 - mae: 2.2586 - val_loss: 16.7205 - val_mae: 1.4307\n",
      "Epoch 2/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 24.9924 - mae: 1.9041\n",
      "Epoch 2: val_loss improved from 16.72049 to 16.27135, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 300ms/step - loss: 24.9234 - mae: 1.9024 - val_loss: 16.2713 - val_mae: 1.4424\n",
      "Epoch 3/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 21.0677 - mae: 1.8395\n",
      "Epoch 3: val_loss improved from 16.27135 to 16.22412, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 299ms/step - loss: 21.1080 - mae: 1.8401 - val_loss: 16.2241 - val_mae: 1.4712\n",
      "Epoch 4/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 20.2049 - mae: 1.7769\n",
      "Epoch 4: val_loss improved from 16.22412 to 15.89088, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 301ms/step - loss: 20.2564 - mae: 1.7784 - val_loss: 15.8909 - val_mae: 1.5747\n",
      "Epoch 5/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 19.2616 - mae: 1.7460\n",
      "Epoch 5: val_loss improved from 15.89088 to 15.50253, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 303ms/step - loss: 19.3241 - mae: 1.7496 - val_loss: 15.5025 - val_mae: 1.5758\n",
      "Epoch 6/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 24.9365 - mae: 2.0662\n",
      "Epoch 6: val_loss improved from 15.50253 to 15.48521, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 301ms/step - loss: 24.8041 - mae: 2.0617 - val_loss: 15.4852 - val_mae: 1.7083\n",
      "Epoch 7/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step - loss: 23.6810 - mae: 2.0525 \n",
      "Epoch 7: val_loss improved from 15.48521 to 14.96836, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 305ms/step - loss: 23.5762 - mae: 2.0487 - val_loss: 14.9684 - val_mae: 1.7296\n",
      "Epoch 8/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - loss: 20.1431 - mae: 1.9192 \n",
      "Epoch 8: val_loss improved from 14.96836 to 14.89478, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 298ms/step - loss: 20.1554 - mae: 1.9203 - val_loss: 14.8948 - val_mae: 1.7410\n",
      "Epoch 9/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 19.6451 - mae: 1.9673\n",
      "Epoch 9: val_loss did not improve from 14.89478\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 298ms/step - loss: 19.6735 - mae: 1.9679 - val_loss: 15.3939 - val_mae: 1.8458\n",
      "Epoch 10/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - loss: 18.9548 - mae: 1.9425\n",
      "Epoch 10: val_loss improved from 14.89478 to 14.69796, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 297ms/step - loss: 18.9979 - mae: 1.9439 - val_loss: 14.6980 - val_mae: 1.7544\n",
      "Epoch 11/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 263ms/step - loss: 18.4587 - mae: 1.9218 \n",
      "Epoch 11: val_loss did not improve from 14.69796\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 294ms/step - loss: 18.5034 - mae: 1.9233 - val_loss: 14.9369 - val_mae: 1.8667\n",
      "Epoch 12/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 18.5695 - mae: 1.9615 \n",
      "Epoch 12: val_loss did not improve from 14.69796\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 297ms/step - loss: 18.6169 - mae: 1.9632 - val_loss: 14.7706 - val_mae: 1.8766\n",
      "Epoch 13/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 19.7616 - mae: 2.0213 \n",
      "Epoch 13: val_loss did not improve from 14.69796\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 298ms/step - loss: 19.7521 - mae: 2.0201 - val_loss: 14.8711 - val_mae: 1.8258\n",
      "Epoch 14/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 19.5788 - mae: 1.9615 \n",
      "Epoch 14: val_loss improved from 14.69796 to 14.60323, saving model to best_convlstm_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 300ms/step - loss: 19.5740 - mae: 1.9623 - val_loss: 14.6032 - val_mae: 1.7845\n",
      "Epoch 15/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 20.5858 - mae: 1.9942\n",
      "Epoch 15: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 298ms/step - loss: 20.5417 - mae: 1.9934 - val_loss: 16.0917 - val_mae: 2.0204\n",
      "Epoch 16/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 18.1909 - mae: 1.9407\n",
      "Epoch 16: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 296ms/step - loss: 18.2131 - mae: 1.9410 - val_loss: 14.9011 - val_mae: 1.9289\n",
      "Epoch 17/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 18.4816 - mae: 1.9409\n",
      "Epoch 17: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 296ms/step - loss: 18.4811 - mae: 1.9404 - val_loss: 14.9791 - val_mae: 1.8309\n",
      "Epoch 18/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 20.1763 - mae: 1.9676 \n",
      "Epoch 18: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 299ms/step - loss: 20.0986 - mae: 1.9652 - val_loss: 15.0748 - val_mae: 1.8514\n",
      "Epoch 19/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 15.3186 - mae: 1.8093\n",
      "Epoch 19: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 297ms/step - loss: 15.4106 - mae: 1.8117 - val_loss: 15.8892 - val_mae: 1.9238\n",
      "Epoch 20/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step - loss: 18.8398 - mae: 1.9473 \n",
      "Epoch 20: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 298ms/step - loss: 18.8183 - mae: 1.9468 - val_loss: 15.2458 - val_mae: 1.8896\n",
      "Epoch 21/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 16.6719 - mae: 1.8741 \n",
      "Epoch 21: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 296ms/step - loss: 16.6885 - mae: 1.8742 - val_loss: 15.3845 - val_mae: 1.9294\n",
      "Epoch 22/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step - loss: 18.1272 - mae: 1.9169 \n",
      "Epoch 22: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 296ms/step - loss: 18.0680 - mae: 1.9142 - val_loss: 15.0832 - val_mae: 1.9083\n",
      "Epoch 23/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step - loss: 17.1114 - mae: 1.8541 \n",
      "Epoch 23: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 300ms/step - loss: 17.0792 - mae: 1.8532 - val_loss: 15.5402 - val_mae: 1.9109\n",
      "Epoch 24/50\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 268ms/step - loss: 16.0607 - mae: 1.8083\n",
      "Epoch 24: val_loss did not improve from 14.60323\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 299ms/step - loss: 16.0630 - mae: 1.8088 - val_loss: 17.1338 - val_mae: 1.9193\n",
      "\n",
      "Model training complete.\n",
      "\n",
      "Evaluating model on the test set...\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 240ms/step - loss: 17.5628 - mae: 2.0386\n",
      "Test Loss (MSE): 15.6500\n",
      "Test MAE: 1.9115\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import ConvLSTM2D, BatchNormalization, Conv3D, Conv2D, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Ensure TensorFlow is using a GPU if available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    print(\"No GPU found. Training on CPU.\")\n",
    "\n",
    "# --- Define Model Parameters based on your data ---\n",
    "# These values should come from your previous data preparation steps\n",
    "# For demonstration purposes, I'll use placeholders if not defined globally.\n",
    "# In a continuous script, these would already be in scope.\n",
    "\n",
    "# Example values (replace with your actual data shapes from previous steps)\n",
    "# from the previous notebook section\n",
    "# X_train_normalized shape: (num_train_samples, n_steps_in, num_lat, num_lon, num_predictor_features)\n",
    "# y_train_normalized shape: (num_train_samples, n_steps_out, num_lat, num_lon)\n",
    "\n",
    "# Assume these are available from the previous run:\n",
    "# X_train_normalized, y_train_normalized, X_test_normalized, y_test_normalized\n",
    "\n",
    "# Infer dimensions from your normalized data\n",
    "n_samples_train, n_steps_in, num_lat, num_lon, num_channels = X_train_normalized.shape\n",
    "_, n_steps_out, _, _ = y_train_normalized.shape # Assuming y_train_normalized has 4 dimensions\n",
    "\n",
    "print(f\"\\nModel Input Parameters:\")\n",
    "print(f\"  n_steps_in (sequence length): {n_steps_in}\")\n",
    "print(f\"  num_lat (grid rows): {num_lat}\")\n",
    "print(f\"  num_lon (grid columns): {num_lon}\")\n",
    "print(f\"  num_channels (input features): {num_channels}\")\n",
    "print(f\"  n_steps_out (prediction horizon): {n_steps_out}\")\n",
    "\n",
    "# --- Adjust y_train_normalized/y_test_normalized if n_steps_out is 1 ---\n",
    "# Keras ConvLSTM2D output can be a sequence or just the last step.\n",
    "# If n_steps_out is 1, it's common to predict a single frame.\n",
    "# The target `y` should match the model's output shape.\n",
    "if n_steps_out == 1:\n",
    "    # Squeeze the n_steps_out dimension from y if it's 1, as the model's Conv3D output will be 3D.\n",
    "    # From (samples, 1, lat, lon) to (samples, lat, lon)\n",
    "    y_train_target = y_train_normalized.squeeze(axis=1)\n",
    "    y_test_target = y_test_normalized.squeeze(axis=1)\n",
    "    print(f\"Target 'y' adjusted from {y_train_normalized.shape} to {y_train_target.shape} for n_steps_out=1.\")\n",
    "else:\n",
    "    y_train_target = y_train_normalized\n",
    "    y_test_target = y_test_normalized\n",
    "    print(f\"Target 'y' shape remains {y_train_target.shape} as n_steps_out > 1.\")\n",
    "\n",
    "# --- Build the ConvLSTM Model ---\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     return_sequences=True,\n",
    "                     input_shape=(n_steps_in, num_lat, num_lon, num_channels)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     return_sequences=False))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(filters=1, kernel_size=(1, 1), activation='linear', padding='same'))\n",
    "\n",
    "if n_steps_out == 1:\n",
    "    model.add(Lambda(lambda x: tf.squeeze(x, axis=-1)))\n",
    "\n",
    "# --- Compile the Model ---\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- Define Callbacks for Training ---\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint('best_convlstm_model.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True,\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(X_train_normalized, y_train_target,\n",
    "                    epochs=50,\n",
    "                    batch_size=8,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "# --- Evaluate the Model on the Test Set ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "test_loss, test_mae = model.evaluate(X_test_normalized, y_test_target, verbose=1)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# You can now use the trained `model` to make predictions:\n",
    "# predictions_normalized = model.predict(X_test_normalized)\n",
    "# Then inverse transform `predictions_normalized` using `scaler_y` to get original precipitation values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redefining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 7 layers, found 5 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[14], line 33\u001b[0m\n",
      "\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Load the saved weights\u001b[39;00m\n",
      "\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Make sure the path matches where you saved your weights\u001b[39;00m\n",
      "\u001b[1;32m     32\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_convlstm_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel weights loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/srsi/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n",
      "\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\n",
      "File \u001b[0;32m~/anaconda3/envs/srsi/lib/python3.9/site-packages/keras/saving/hdf5_format.py:728\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, model)\u001b[0m\n",
      "\u001b[1;32m    726\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n",
      "\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n",
      "\u001b[0;32m--> 728\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[1;32m    729\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m    730\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;32m    731\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n",
      "\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n",
      "\u001b[1;32m    735\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\n",
      "\u001b[0;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 7 layers, found 5 saved layers."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Flatten, Dense\n",
    "\n",
    "n_steps_in = 5\n",
    "n_features_in = 8\n",
    "n_rows = 41\n",
    "n_cols = 61\n",
    "n_output_features = n_rows * n_cols\n",
    "\n",
    "# Re-define your model architecture (MUST be identical to the one you trained)\n",
    "# This is a hypothetical example; adapt it to your actual model structure\n",
    "model = Sequential([\n",
    "    ConvLSTM2D(filters=40, kernel_size=(3, 3), activation='relu',\n",
    "               input_shape=(n_steps_in, n_features_in, n_rows, n_cols),\n",
    "               padding='same', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    ConvLSTM2D(filters=40, kernel_size=(3, 3), activation='relu',\n",
    "               padding='same', return_sequences=True),\n",
    "    BatchNormalization(),\n",
    "    ConvLSTM2D(filters=40, kernel_size=(3, 3), activation='relu',\n",
    "               padding='same', return_sequences=False),\n",
    "    BatchNormalization(),\n",
    "    Flatten(),\n",
    "    Dense(n_output_features) # n_output_features should match your target shape\n",
    "])\n",
    "\n",
    "# Compile the model (necessary before loading weights for some TensorFlow versions, or before resuming training)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Load the saved weights\n",
    "# Make sure the path matches where you saved your weights\n",
    "weights_path = 'best_convlstm_model.h5'\n",
    "model.load_weights(weights_path)\n",
    "\n",
    "print(\"Model weights loaded successfully!\")\n",
    "\n",
    "# Now you can use the model for predictions\n",
    "# predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m predictions_normalized \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test_normalized)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "predictions_normalized = model.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m num_samples, lat, lon \u001b[38;5;241m=\u001b[39m \u001b[43mpredictions_normalized\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;32m      2\u001b[0m preds_reshaped \u001b[38;5;241m=\u001b[39m predictions_normalized\u001b[38;5;241m.\u001b[39mreshape(num_samples, lat \u001b[38;5;241m*\u001b[39m lon)\n",
      "\u001b[1;32m      4\u001b[0m preds_inversed \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(preds_reshaped)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "    num_samples, lat, lon = predictions_normalized.shape\n",
    "    preds_reshaped = predictions_normalized.reshape(num_samples, lat * lon)\n",
    "\n",
    "    preds_inversed = scaler_y.inverse_transform(preds_reshaped)\n",
    "    preds_inversed = preds_inversed.reshape(num_samples, lat, lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds_inversed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Select sample index to plot\u001b[39;00m\n",
      "\u001b[1;32m      5\u001b[0m sample_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;32m----> 7\u001b[0m pred_to_plot \u001b[38;5;241m=\u001b[39m \u001b[43mpreds_inversed\u001b[49m[sample_idx]  \u001b[38;5;66;03m# shape (lat, lon)\u001b[39;00m\n",
      "\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(pred_to_plot, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preds_inversed' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select sample index to plot\n",
    "sample_idx = 20\n",
    "\n",
    "pred_to_plot = preds_inversed[sample_idx]  # shape (lat, lon)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(pred_to_plot, origin='lower', cmap='Blues')\n",
    "plt.colorbar(label='Total precipitation (units)')\n",
    "plt.title(f'Predicted precipitation (sample {sample_idx}, step {time_step if n_steps_out > 1 else \"\"})')\n",
    "plt.xlabel('Longitude grid index')\n",
    "plt.ylabel('Latitude grid index')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m y_test_reshaped \u001b[38;5;241m=\u001b[39m \u001b[43my_test_normalized\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(y_test_normalized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], num_lat \u001b[38;5;241m*\u001b[39m num_lon)\n",
      "\u001b[1;32m      2\u001b[0m y_test_inversed \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(y_test_reshaped)\n",
      "\u001b[1;32m      3\u001b[0m y_test_inversed \u001b[38;5;241m=\u001b[39m y_test_inversed\u001b[38;5;241m.\u001b[39mreshape(y_test_normalized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], num_lat, num_lon)\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "y_test_reshaped = y_test_normalized.reshape(y_test_normalized.shape[0], num_lat * num_lon)\n",
    "y_test_inversed = scaler_y.inverse_transform(y_test_reshaped)\n",
    "y_test_inversed = y_test_inversed.reshape(y_test_normalized.shape[0], num_lat, num_lon)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample_idx = 70  # Which test sample to visualize\n",
    "time_step = 0   # Which time step to visualize (if multiple)\n",
    "\n",
    "if n_steps_out == 1:\n",
    "    pred_to_plot = preds_inversed[sample_idx]\n",
    "    true_to_plot = y_test_inversed[sample_idx]\n",
    "else:\n",
    "    pred_to_plot = preds_inversed[sample_idx, time_step]\n",
    "    true_to_plot = y_test_inversed[sample_idx, time_step]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "im1 = axes[0].imshow(true_to_plot, origin='lower', cmap='Blues')\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[0].axis('off')\n",
    "fig.colorbar(im1, ax=axes[0], fraction=0.046, pad=0.04)\n",
    "\n",
    "im2 = axes[1].imshow(pred_to_plot, origin='lower', cmap='Blues')\n",
    "axes[1].set_title('Prediction')\n",
    "axes[1].axis('off')\n",
    "fig.colorbar(im2, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.suptitle(f'Precipitation comparison (sample {sample_idx}, step {time_step if n_steps_out > 1 else \"\"})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_to_plot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m error_map \u001b[38;5;241m=\u001b[39m \u001b[43mpred_to_plot\u001b[49m \u001b[38;5;241m-\u001b[39m true_to_plot\n",
      "\u001b[1;32m      3\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m all_times[sample_idx \u001b[38;5;241m+\u001b[39m n_steps_in \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_to_plot' is not defined"
     ]
    }
   ],
   "source": [
    "error_map = pred_to_plot - true_to_plot\n",
    "\n",
    "timestamp = all_times[sample_idx + n_steps_in - 1]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(error_map, origin='lower', cmap='bwr', vmin=-abs(error_map).max(), vmax=abs(error_map).max())\n",
    "plt.colorbar(label='Prediction error (pred - true)')\n",
    "plt.title(f'Prediction error (sample {sample_idx}, step {time_step if n_steps_out > 1 else \"\"})')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
