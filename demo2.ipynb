{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4553d129-a65b-4b83-b9f4-ba7896193798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from prophet import Prophet\n",
    "import cftime\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import seaborn as sns # for plotting\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7bbd65-46df-4c20-8a12-dc9175b43c7e",
   "metadata": {},
   "source": [
    "# Extracting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83bda818-f999-494e-b0a6-186ece405959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 17GB\n",
      "Dimensions:     (valid_time: 365, latitude: 721, longitude: 1440)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 2002-01-01T12:00:00 ... 2002-...\n",
      "  * latitude    (latitude) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * longitude   (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 6kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    mwd         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    mwp         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    swh         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    u10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    d2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    msl         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sst         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-07-05T14:27 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "file_paths = ['CDS_Data/data_stream-wave_stepType-instant.nc', 'CDS_Data/data_stream-oper_stepType-instant.nc', \n",
    "           'CDS_Data/data_stream-oper_stepType-accum.nc']\n",
    "\n",
    "# Create an empty list to store the Datasets\n",
    "datasets = []\n",
    "\n",
    "# Open each NetCDF file and append its Dataset to the list\n",
    "for fp in file_paths:\n",
    "    ds = xr.open_dataset(fp)\n",
    "    datasets.append(ds)\n",
    "\n",
    "# Merge all Datasets in the list into a single Dataset\n",
    "combined_dataset = xr.merge(datasets)\n",
    "\n",
    "# Inspect your dataset\n",
    "print(combined_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07aa09d3-76a5-4489-b545-e958a4536b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Checking Latitude Coordinates ---\n",
      "<xarray.DataArray 'latitude' (latitude: 721)> Size: 6kB\n",
      "array([-90.  , -89.75, -89.5 , ...,  89.5 ,  89.75,  90.  ])\n",
      "Coordinates:\n",
      "  * latitude  (latitude) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "    number    int64 8B 0\n",
      "Attributes:\n",
      "    units:             degrees_north\n",
      "    standard_name:     latitude\n",
      "    long_name:         latitude\n",
      "    stored_direction:  decreasing\n",
      "Minimum Latitude: -90.0\n",
      "Maximum Latitude: 90.0\n",
      "\n",
      "--- Checking Longitude Coordinates ---\n",
      "<xarray.DataArray 'longitude' (longitude: 1440)> Size: 12kB\n",
      "array([0.0000e+00, 2.5000e-01, 5.0000e-01, ..., 3.5925e+02, 3.5950e+02,\n",
      "       3.5975e+02])\n",
      "Coordinates:\n",
      "  * longitude  (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "    number     int64 8B 0\n",
      "Attributes:\n",
      "    units:          degrees_east\n",
      "    standard_name:  longitude\n",
      "    long_name:      longitude\n",
      "Minimum Longitude: 0.0\n",
      "Maximum Longitude: 359.75\n",
      "\n",
      "--- Full Dataset Info ---\n",
      "<xarray.Dataset> Size: 17GB\n",
      "Dimensions:     (valid_time: 365, latitude: 721, longitude: 1440)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 2002-01-01T12:00:00 ... 2002-...\n",
      "  * latitude    (latitude) float64 6kB -90.0 -89.75 -89.5 ... 89.5 89.75 90.0\n",
      "  * longitude   (longitude) float64 12kB 0.0 0.25 0.5 0.75 ... 359.2 359.5 359.8\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 6kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    mwd         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    mwp         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    swh         (valid_time, latitude, longitude) float32 2GB nan nan ... nan\n",
      "    u10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    d2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    msl         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sst         (valid_time, latitude, longitude) float32 2GB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 2GB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-07-05T14:27 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Checking Latitude Coordinates ---\")\n",
    "print(combined_dataset['latitude'])\n",
    "print(f\"Minimum Latitude: {combined_dataset['latitude'].min().item()}\")\n",
    "print(f\"Maximum Latitude: {combined_dataset['latitude'].max().item()}\")\n",
    "\n",
    "print(\"\\n--- Checking Longitude Coordinates ---\")\n",
    "print(combined_dataset['longitude'])\n",
    "print(f\"Minimum Longitude: {combined_dataset['longitude'].min().item()}\")\n",
    "print(f\"Maximum Longitude: {combined_dataset['longitude'].max().item()}\")\n",
    "\n",
    "print(\"\\n--- Full Dataset Info ---\")\n",
    "print(combined_dataset) # Look closely at the dimensions here too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3b36099-dcdb-4ef0-ae6f-0f170f4c6a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 40MB\n",
      "Dimensions:     (valid_time: 365, latitude: 41, longitude: 61)\n",
      "Coordinates:\n",
      "  * valid_time  (valid_time) datetime64[ns] 3kB 2002-01-01T12:00:00 ... 2002-...\n",
      "  * latitude    (latitude) float64 328B 45.0 45.25 45.5 ... 54.5 54.75 55.0\n",
      "  * longitude   (longitude) float64 488B 5.0 5.25 5.5 5.75 ... 19.5 19.75 20.0\n",
      "    number      int64 8B 0\n",
      "    expver      (valid_time) <U4 6kB '0001' '0001' '0001' ... '0001' '0001'\n",
      "Data variables:\n",
      "    mwd         (valid_time, latitude, longitude) float32 4MB nan nan ... 344.9\n",
      "    mwp         (valid_time, latitude, longitude) float32 4MB nan nan ... 6.452\n",
      "    swh         (valid_time, latitude, longitude) float32 4MB nan nan ... 2.092\n",
      "    u10         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    v10         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    d2m         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    t2m         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    msl         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    sst         (valid_time, latitude, longitude) float32 4MB ...\n",
      "    sp          (valid_time, latitude, longitude) float32 4MB ...\n",
      "    tp          (valid_time, latitude, longitude) float32 4MB ...\n",
      "Attributes:\n",
      "    GRIB_centre:             ecmf\n",
      "    GRIB_centreDescription:  European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre:          0\n",
      "    Conventions:             CF-1.7\n",
      "    institution:             European Centre for Medium-Range Weather Forecasts\n",
      "    history:                 2025-07-05T14:27 GRIB to CDM+CF via cfgrib-0.9.1...\n"
     ]
    }
   ],
   "source": [
    "north_bound_lat_chch = 55.0  # Remember: -43.0 is \"more north\" than -44.0\n",
    "south_bound_lat_chch = 45.0\n",
    "west_bound_lon_chch = 5.0\n",
    "east_bound_lon_chch = 20.0\n",
    "\n",
    "# 1. Select the data for the specified Christchurch region\n",
    "# IMPORTANT: For 'latitude' which is 'decreasing' (90 to -90), the slice order is (higher_value, lower_value)\n",
    "ds = combined_dataset.sel(\n",
    "    latitude=slice(south_bound_lat_chch, north_bound_lat_chch), # Correct order for decreasing latitude\n",
    "    longitude=slice(west_bound_lon_chch, east_bound_lon_chch)   # Standard order for increasing longitude (0 to 359.5)\n",
    ")\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91be01f9-ea4e-40cf-8dac-d02abad65cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        number expver         mwd       mwp  \\\n",
      "valid_time          latitude longitude                                        \n",
      "2002-01-01 12:00:00 45.0     5.00            0   0001         NaN       NaN   \n",
      "                             5.25            0   0001         NaN       NaN   \n",
      "                             5.50            0   0001         NaN       NaN   \n",
      "                             5.75            0   0001         NaN       NaN   \n",
      "                             6.00            0   0001         NaN       NaN   \n",
      "...                                        ...    ...         ...       ...   \n",
      "2002-12-31 12:00:00 55.0     19.00           0   0001  351.784027  6.344384   \n",
      "                             19.25           0   0001         NaN       NaN   \n",
      "                             19.50           0   0001  347.205902  6.363671   \n",
      "                             19.75           0   0001         NaN       NaN   \n",
      "                             20.00           0   0001  344.869965  6.451562   \n",
      "\n",
      "                                             swh       u10       v10  \\\n",
      "valid_time          latitude longitude                                 \n",
      "2002-01-01 12:00:00 45.0     5.00            NaN -0.804764 -1.102310   \n",
      "                             5.25            NaN -0.738358 -0.229263   \n",
      "                             5.50            NaN -0.763748  0.397690   \n",
      "                             5.75            NaN -1.006912  0.909409   \n",
      "                             6.00            NaN -1.389725  0.598862   \n",
      "...                                          ...       ...       ...   \n",
      "2002-12-31 12:00:00 55.0     19.00      2.196946  5.939423 -8.245361   \n",
      "                             19.25           NaN  6.187469 -8.601807   \n",
      "                             19.50      2.306565  6.205048 -8.883057   \n",
      "                             19.75           NaN  5.865204 -8.530518   \n",
      "                             20.00      2.091721  4.832001 -6.674072   \n",
      "\n",
      "                                               d2m         t2m          msl  \\\n",
      "valid_time          latitude longitude                                        \n",
      "2002-01-01 12:00:00 45.0     5.00       267.758881  274.061951  103693.1250   \n",
      "                             5.25       266.268646  273.729919  103658.1250   \n",
      "                             5.50       265.891693  271.888123  103644.6250   \n",
      "                             5.75       261.844818  267.663513  103620.3750   \n",
      "                             6.00       256.305756  261.890076  103619.1250   \n",
      "...                                            ...         ...          ...   \n",
      "2002-12-31 12:00:00 55.0     19.00      265.124939  271.457428  101757.9375   \n",
      "                             19.25      265.246033  271.527740  101729.1875   \n",
      "                             19.50      265.076111  271.633209  101700.4375   \n",
      "                             19.75      264.966736  271.465240  101673.6875   \n",
      "                             20.00      265.292908  270.521881  101650.9375   \n",
      "\n",
      "                                               sst             sp        tp  \n",
      "valid_time          latitude longitude                                       \n",
      "2002-01-01 12:00:00 45.0     5.00              NaN   99381.828125  0.000000  \n",
      "                             5.25              NaN   96759.828125  0.000000  \n",
      "                             5.50              NaN   93320.828125  0.000000  \n",
      "                             5.75              NaN   88884.828125  0.000000  \n",
      "                             6.00              NaN   84657.828125  0.000000  \n",
      "...                                            ...            ...       ...  \n",
      "2002-12-31 12:00:00 55.0     19.00      276.916260  101738.914062  0.000000  \n",
      "                             19.25      276.927002  101705.914062  0.000002  \n",
      "                             19.50      276.882080  101663.914062  0.000012  \n",
      "                             19.75      276.850830  101622.914062  0.000017  \n",
      "                             20.00             NaN  101561.914062  0.000015  \n",
      "\n",
      "[912865 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "df = ds.to_dataframe()\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b760586-5118-45bf-b17f-ae5c50f5456f",
   "metadata": {},
   "source": [
    "# Addressing NaNs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c33a9c17-661b-4c9f-9358-d67fda97a606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NaN counts per column in DataFrame:\n",
      "number         0\n",
      "expver         0\n",
      "mwd       884395\n",
      "mwp       884395\n",
      "swh       884395\n",
      "u10            0\n",
      "v10            0\n",
      "d2m            0\n",
      "t2m            0\n",
      "msl            0\n",
      "sst       821250\n",
      "sp             0\n",
      "tp             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNaN counts per column in DataFrame:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51ca66b8-9c78-4168-9e72-3613bab4cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number    0\n",
      "expver    0\n",
      "u10       0\n",
      "v10       0\n",
      "d2m       0\n",
      "t2m       0\n",
      "msl       0\n",
      "sp        0\n",
      "tp        0\n",
      "dtype: int64\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# List of columns to drop\n",
    "columns_to_drop = ['mwd', 'mwp', 'swh', 'sst']\n",
    "\n",
    "# Drop the columns\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(print(df_cleaned.isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085d8e3-d7d4-4a41-bdf1-1c7af48b4662",
   "metadata": {},
   "source": [
    "# Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2bb5aff5-8e7c-4337-b8f5-7f492a49770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame after sorting and re-indexing (first few rows):\n",
      "                                        number expver       u10       v10  \\\n",
      "valid_time          latitude longitude                                      \n",
      "2002-01-01 12:00:00 45.0     5.00            0   0001 -0.804764 -1.102310   \n",
      "                             5.25            0   0001 -0.738358 -0.229263   \n",
      "                             5.50            0   0001 -0.763748  0.397690   \n",
      "                             5.75            0   0001 -1.006912  0.909409   \n",
      "                             6.00            0   0001 -1.389725  0.598862   \n",
      "\n",
      "                                               d2m         t2m         msl  \\\n",
      "valid_time          latitude longitude                                       \n",
      "2002-01-01 12:00:00 45.0     5.00       267.758881  274.061951  103693.125   \n",
      "                             5.25       266.268646  273.729919  103658.125   \n",
      "                             5.50       265.891693  271.888123  103644.625   \n",
      "                             5.75       261.844818  267.663513  103620.375   \n",
      "                             6.00       256.305756  261.890076  103619.125   \n",
      "\n",
      "                                                  sp   tp  \n",
      "valid_time          latitude longitude                     \n",
      "2002-01-01 12:00:00 45.0     5.00       99381.828125  0.0  \n",
      "                             5.25       96759.828125  0.0  \n",
      "                             5.50       93320.828125  0.0  \n",
      "                             5.75       88884.828125  0.0  \n",
      "                             6.00       84657.828125  0.0  \n",
      "Original shape: (912865, 9)\n"
     ]
    }
   ],
   "source": [
    "df_reset = df_cleaned.reset_index()\n",
    "\n",
    "# Convert 'valid_time' to datetime\n",
    "df_reset['valid_time'] = pd.to_datetime(df_reset['valid_time'])\n",
    "\n",
    "# Sort by time, then latitude, then longitude to ensure consistent grid formation\n",
    "df_sorted = df_reset.sort_values(by=['valid_time', 'latitude', 'longitude'])\n",
    "\n",
    "# Re-set the MultiIndex for easier handling, though we might iterate through it\n",
    "df_final = df_sorted.set_index(['valid_time', 'latitude', 'longitude'])\n",
    "\n",
    "print(\"DataFrame after sorting and re-indexing (first few rows):\")\n",
    "print(df_final.head())\n",
    "print(f\"Original shape: {df_final.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "694802d3-4b78-46e6-a029-30c720dd7ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grid dimensions: Latitude=41, Longitude=61\n",
      "Number of features (variables): 9\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Determine Grid Dimensions ---\n",
    "# Get unique latitudes and longitudes\n",
    "latitudes = df_final.index.get_level_values('latitude').unique().sort_values()\n",
    "longitudes = df_final.index.get_level_values('longitude').unique().sort_values()\n",
    "\n",
    "num_lat = len(latitudes)\n",
    "num_lon = len(longitudes)\n",
    "num_features = len(df_final.columns) # All columns are features except if we designate one as target later\n",
    "\n",
    "print(f\"\\nGrid dimensions: Latitude={num_lat}, Longitude={num_lon}\")\n",
    "print(f\"Number of features (variables): {num_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e24f933-b43e-4213-a5e7-90342ef27afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of gridded_data: (365, 41, 61, 9)\n",
      "NaNs in gridded data after initial reshape: 0\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Reshape to Gridded Data (Time, Lat, Lon, Features) ---\n",
    "# Get all unique timestamps\n",
    "all_times = df_final.index.get_level_values('valid_time').unique().sort_values()\n",
    "num_times = len(all_times)\n",
    "\n",
    "# Create an empty array to store the gridded data\n",
    "# Shape: (num_times, num_lat, num_lon, num_features)\n",
    "gridded_data = np.full((num_times, num_lat, num_lon, num_features), np.nan) # Use nan for missing spots\n",
    "\n",
    "# Map latitude and longitude to array indices\n",
    "lat_to_idx = {lat: i for i, lat in enumerate(latitudes)}\n",
    "lon_to_idx = {lon: i for i, lon in enumerate(longitudes)}\n",
    "\n",
    "# Populate the gridded_data array\n",
    "for t_idx, time in enumerate(all_times):\n",
    "    # Select data for the current timestamp\n",
    "    time_slice_df = df_final.loc[time]\n",
    "\n",
    "    for (lat, lon), row_data in time_slice_df.iterrows():\n",
    "        lat_idx = lat_to_idx[lat]\n",
    "        lon_idx = lon_to_idx[lon]\n",
    "        gridded_data[t_idx, lat_idx, lon_idx, :] = row_data.values\n",
    "\n",
    "print(f\"\\nShape of gridded_data: {gridded_data.shape}\")\n",
    "# Check if there are any NaNs left from this reshaping.\n",
    "# There shouldn't be if all combinations of lat/lon exist for each time.\n",
    "print(f\"NaNs in gridded data after initial reshape: {np.sum(np.isnan(gridded_data))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f15679f7-ae0b-45ed-a40b-582754ad6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define Input (X) and Target (y) Variables ---\n",
    "# 'tp' is total precipitation, which we want to predict.\n",
    "# Let's find its column index.\n",
    "feature_names = df_final.columns.tolist()\n",
    "tp_col_idx = feature_names.index('tp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ee8b027-25bc-46a2-a430-fd525a466a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of X_sequences (input): (360, 5, 41, 61, 8)\n",
      "Shape of y_sequences (target): (360, 1, 41, 61)\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Create Sequences for ConvLSTM ---\n",
    "# We need to define:\n",
    "# - `n_steps_in`: how many past time steps to use as input\n",
    "# - `n_steps_out`: how many future time steps to predict (for rainfall, often 1)\n",
    "\n",
    "n_steps_in = 5  # Example: Use the past 5 time steps (e.g., 5 days if data is daily)\n",
    "n_steps_out = 1 # Example: Predict the next 1 time step (e.g., next day's rainfall)\n",
    "\n",
    "def create_sequences(data, n_steps_in, n_steps_out, tp_idx):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - n_steps_in - n_steps_out + 1):\n",
    "        # Input sequence (all features except 'tp' for n_steps_in)\n",
    "        # We need to exclude 'tp' from the input features\n",
    "        X_seq_features = np.delete(data[i:(i + n_steps_in)], tp_idx, axis=-1)\n",
    "        X.append(X_seq_features)\n",
    "\n",
    "        # Output sequence (only 'tp' for n_steps_out)\n",
    "        y_seq_tp = data[(i + n_steps_in):(i + n_steps_in + n_steps_out), :, :, tp_idx]\n",
    "        y.append(y_seq_tp)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_sequences, y_sequences = create_sequences(gridded_data, n_steps_in, n_steps_out, tp_col_idx)\n",
    "\n",
    "print(f\"\\nShape of X_sequences (input): {X_sequences.shape}\")\n",
    "print(f\"Shape of y_sequences (target): {y_sequences.shape}\")\n",
    "\n",
    "# X_sequences shape should be (num_samples, n_steps_in, num_lat, num_lon, num_features - 1)\n",
    "# y_sequences shape should be (num_samples, n_steps_out, num_lat, num_lon) - if predicting one variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8ec918d-846a-4b89-9f1e-dfc408bbd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set size: 288 sequences\n",
      "Test set size: 72 sequences\n",
      "X_train shape: (288, 5, 41, 61, 8)\n",
      "y_train shape: (288, 1, 41, 61)\n",
      "X_test shape: (72, 5, 41, 61, 8)\n",
      "y_test shape: (72, 1, 41, 61)\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Time-based Train-Test Split ---\n",
    "# The data covers 2002. Let's use the first ~9 months for training and the last ~3 months for testing.\n",
    "# A common split is 80% train / 20% test.\n",
    "# Since X_sequences and y_sequences are already ordered by time, we can simply split the arrays.\n",
    "\n",
    "# Determine the split point based on the number of sequences\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X_sequences) * split_ratio)\n",
    "\n",
    "X_train = X_sequences[:split_index]\n",
    "y_train = y_sequences[:split_index]\n",
    "X_test = X_sequences[split_index:]\n",
    "y_test = y_sequences[split_index:]\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)} sequences\")\n",
    "print(f\"Test set size: {len(X_test)} sequences\")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cb6f03-a248-4d88-a6a2-3824ae4d3708",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "235ae4a9-fbf8-4a1e-b039-86a6bf265261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (channels) in X_sequences before normalization: ['number', 'expver', 'u10', 'v10', 'd2m', 't2m', 'msl', 'sp']\n",
      "Number of features (channels) in X_sequences: 8\n"
     ]
    }
   ],
   "source": [
    "original_feature_names_in_X_sequences = [col for col in df_final.columns if col != 'tp']\n",
    "\n",
    "\n",
    "print(f\"Features (channels) in X_sequences before normalization: {original_feature_names_in_X_sequences}\")\n",
    "print(f\"Number of features (channels) in X_sequences: {len(original_feature_names_in_X_sequences)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1a9b521-bc09-4c62-aca3-00532db25adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Feature 'number' is constant and will not be effectively normalized by RobustScaler.\n",
      "It's generally recommended to remove constant features before creating sequences if they are truly constant.\n",
      "Warning: Feature 'expver' is constant and will not be effectively normalized by RobustScaler.\n",
      "It's generally recommended to remove constant features before creating sequences if they are truly constant.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Identify and Exclude Constant Features (if any) ---\n",
    "# This step should ideally be done BEFORE creating X_sequences.\n",
    "# Let's check for constants in the original `df_final` used to create the gridded data.\n",
    "# If df_final is not available, you would need to load a sample or assume.\n",
    "\n",
    "constant_features_found = []\n",
    "if 'df_final' in locals():\n",
    "    # Check only features that ended up in X_sequences\n",
    "    for col in original_feature_names_in_X_sequences:\n",
    "        if df_final[col].nunique() == 1:\n",
    "            constant_features_found.append(col)\n",
    "            print(f\"Warning: Feature '{col}' is constant and will not be effectively normalized by RobustScaler.\")\n",
    "            print(f\"It's generally recommended to remove constant features before creating sequences if they are truly constant.\")\n",
    "else:\n",
    "    print(\"Warning: `df_final` not found. Cannot verify if features are constant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db7325ff-f140-41db-b0d4-230224a4390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train reshaped for scaler fitting: (3601440, 8)\n",
      "\n",
      "RobustScaler for features (X) fitted on X_train data.\n",
      "X_train normalized reshaped shape: (3601440, 8)\n",
      "X_test normalized reshaped shape: (900360, 8)\n",
      "\n",
      "Final X_train_normalized shape (ready for ConvLSTM input): (288, 5, 41, 61, 8)\n",
      "Final X_test_normalized shape (ready for ConvLSTM input): (72, 5, 41, 61, 8)\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Normalization of X_train and X_test features ---\n",
    "\n",
    "# Get the shape of X_train to reshape back later\n",
    "shape_x_train = X_train.shape # (num_samples, n_steps_in, num_lat, num_lon, num_features_for_scaling)\n",
    "num_features_for_scaling = shape_x_train[-1] # Number of channels (e.g., 8)\n",
    "\n",
    "# Flatten X_train for scaler fitting: (total_elements, num_features)\n",
    "# This combines all samples, timesteps, lat, and lon into a single dimension for feature scaling\n",
    "X_train_reshaped_for_scaler = X_train.reshape(-1, num_features_for_scaling)\n",
    "print(f\"\\nX_train reshaped for scaler fitting: {X_train_reshaped_for_scaler.shape}\")\n",
    "\n",
    "# Initialize RobustScaler for features\n",
    "scaler_X = RobustScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data's reshaped features\n",
    "scaler_X.fit(X_train_reshaped_for_scaler)\n",
    "print(\"\\nRobustScaler for features (X) fitted on X_train data.\")\n",
    "\n",
    "# Transform both training and test data using the fitted scaler\n",
    "X_train_normalized_reshaped = scaler_X.transform(X_train_reshaped_for_scaler)\n",
    "X_test_normalized_reshaped = scaler_X.transform(X_test.reshape(-1, num_features_for_scaling)) # Reshape X_test on the fly\n",
    "\n",
    "print(f\"X_train normalized reshaped shape: {X_train_normalized_reshaped.shape}\")\n",
    "print(f\"X_test normalized reshaped shape: {X_test_normalized_reshaped.shape}\")\n",
    "\n",
    "# Reshape the normalized data back to the ConvLSTM input format (5D)\n",
    "X_train_normalized = X_train_normalized_reshaped.reshape(shape_x_train)\n",
    "X_test_normalized = X_test_normalized_reshaped.reshape(X_test.shape) # Use X_test's original shape\n",
    "\n",
    "print(f\"\\nFinal X_train_normalized shape (ready for ConvLSTM input): {X_train_normalized.shape}\")\n",
    "print(f\"Final X_test_normalized shape (ready for ConvLSTM input): {X_test_normalized.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "649056b0-7a7f-4155-9d30-90f9c30a08cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "y_train reshaped for scaler fitting: (720288, 1)\n",
      "RobustScaler for target (y/tp) fitted on y_train data.\n",
      "Final y_train_normalized shape (ready for ConvLSTM output): (288, 1, 41, 61)\n",
      "Final y_test_normalized shape (ready for ConvLSTM output): (72, 1, 41, 61)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Normalization for the Target Variable (y - Total Precipitation 'tp') ---\n",
    "# 'tp' values are non-negative and often have a high concentration at zero, with some large values.\n",
    "# RobustScaler can work, but consider alternatives like log1p if zeros are dominant and you want to reduce skewness.\n",
    "\n",
    "# Get the shape of y_train to reshape back later\n",
    "shape_y_train = y_train.shape # (num_samples, n_steps_out, num_lat, num_lon)\n",
    "num_features_y = 1 # 'tp' is a single target variable\n",
    "\n",
    "# Flatten y_train for scaler fitting: (total_elements, 1)\n",
    "y_train_reshaped_for_scaler = y_train.reshape(-1, num_features_y)\n",
    "print(f\"\\ny_train reshaped for scaler fitting: {y_train_reshaped_for_scaler.shape}\")\n",
    "\n",
    "# Initialize RobustScaler for the target 'tp'\n",
    "scaler_y = RobustScaler()\n",
    "\n",
    "# Fit the scaler ONLY on the training data's reshaped target\n",
    "scaler_y.fit(y_train_reshaped_for_scaler)\n",
    "print(\"RobustScaler for target (y/tp) fitted on y_train data.\")\n",
    "\n",
    "# Transform both training and test target data\n",
    "y_train_normalized_reshaped = scaler_y.transform(y_train_reshaped_for_scaler)\n",
    "y_test_normalized_reshaped = scaler_y.transform(y_test.reshape(-1, num_features_y))\n",
    "\n",
    "y_train_normalized = y_train_normalized_reshaped.reshape(shape_y_train)\n",
    "y_test_normalized = y_test_normalized_reshaped.reshape(y_test.shape)\n",
    "\n",
    "print(f\"Final y_train_normalized shape (ready for ConvLSTM output): {y_train_normalized.shape}\")\n",
    "print(f\"Final y_test_normalized shape (ready for ConvLSTM output): {y_test_normalized.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093c44de-3b1a-4b08-b706-fcd8c9b4037b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1a1646-06b2-470c-bc98-fa6b0fe03ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n",
      "No GPU found. Training on CPU.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_normalized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo GPU found. Training on CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# --- Define Model Parameters based on your data ---\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# These values should come from your previous data preparation steps\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# For demonstration purposes, I'll use placeholders if not defined globally.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Infer dimensions from your normalized data\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m n_samples_train, n_steps_in, num_lat, num_lon, num_channels \u001b[38;5;241m=\u001b[39m \u001b[43mX_train_normalized\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     29\u001b[0m _, n_steps_out, _, _ \u001b[38;5;241m=\u001b[39m y_train_normalized\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# Assuming y_train_normalized has 4 dimensions\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Input Parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_normalized' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, BatchNormalization, Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Ensure TensorFlow is using a GPU if available\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "if len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n",
    "    print(\"Using GPU for training.\")\n",
    "else:\n",
    "    print(\"No GPU found. Training on CPU.\")\n",
    "\n",
    "# --- Define Model Parameters based on your data ---\n",
    "# These values should come from your previous data preparation steps\n",
    "# For demonstration purposes, I'll use placeholders if not defined globally.\n",
    "# In a continuous script, these would already be in scope.\n",
    "\n",
    "# Example values (replace with your actual data shapes from previous steps)\n",
    "# from the previous notebook section\n",
    "# X_train_normalized shape: (num_train_samples, n_steps_in, num_lat, num_lon, num_predictor_features)\n",
    "# y_train_normalized shape: (num_train_samples, n_steps_out, num_lat, num_lon)\n",
    "\n",
    "# Assume these are available from the previous run:\n",
    "# X_train_normalized, y_train_normalized, X_test_normalized, y_test_normalized\n",
    "\n",
    "# Infer dimensions from your normalized data\n",
    "n_samples_train, n_steps_in, num_lat, num_lon, num_channels = X_train_normalized.shape\n",
    "_, n_steps_out, _, _ = y_train_normalized.shape # Assuming y_train_normalized has 4 dimensions\n",
    "\n",
    "print(f\"\\nModel Input Parameters:\")\n",
    "print(f\"  n_steps_in (sequence length): {n_steps_in}\")\n",
    "print(f\"  num_lat (grid rows): {num_lat}\")\n",
    "print(f\"  num_lon (grid columns): {num_lon}\")\n",
    "print(f\"  num_channels (input features): {num_channels}\")\n",
    "print(f\"  n_steps_out (prediction horizon): {n_steps_out}\")\n",
    "\n",
    "# --- Adjust y_train_normalized/y_test_normalized if n_steps_out is 1 ---\n",
    "# Keras ConvLSTM2D output can be a sequence or just the last step.\n",
    "# If n_steps_out is 1, it's common to predict a single frame.\n",
    "# The target `y` should match the model's output shape.\n",
    "if n_steps_out == 1:\n",
    "    # Squeeze the n_steps_out dimension from y if it's 1, as the model's Conv3D output will be 3D.\n",
    "    # From (samples, 1, lat, lon) to (samples, lat, lon)\n",
    "    y_train_target = y_train_normalized.squeeze(axis=1)\n",
    "    y_test_target = y_test_normalized.squeeze(axis=1)\n",
    "    print(f\"Target 'y' adjusted from {y_train_normalized.shape} to {y_train_target.shape} for n_steps_out=1.\")\n",
    "else:\n",
    "    y_train_target = y_train_normalized\n",
    "    y_test_target = y_test_normalized\n",
    "    print(f\"Target 'y' shape remains {y_train_target.shape} as n_steps_out > 1.\")\n",
    "\n",
    "\n",
    "# --- Build the ConvLSTM Model ---\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1: ConvLSTM2D\n",
    "# filters: Number of convolutional filters. More filters can capture more patterns.\n",
    "# kernel_size: The dimensions of the convolution window (e.g., (3,3)).\n",
    "# activation: Activation function. 'relu' is common.\n",
    "# padding: 'same' to maintain spatial dimensions, 'valid' to reduce.\n",
    "# return_sequences: True to stack another ConvLSTM2D layer, False for the last ConvLSTM2D.\n",
    "# input_shape: (n_steps_in, num_lat, num_lon, num_channels)\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     return_sequences=True, # Return sequences if you plan to stack more ConvLSTM2D\n",
    "                     input_shape=(n_steps_in, num_lat, num_lon, num_channels)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Layer 2: ConvLSTM2D (Optional - you can add more or fewer)\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     return_sequences=False)) # Set to False for the last ConvLSTM2D to output a single sequence item\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Output Layer: Conv3D for predicting a sequence of 2D grids OR Conv2D if n_steps_out=1\n",
    "# Since we designed for n_steps_out = 1 and squeezed y, we need an output that is (lat, lon)\n",
    "# After the last ConvLSTM2D (return_sequences=False), the output is (batch_size, num_lat, num_lon, filters)\n",
    "# We need to transform this to (batch_size, num_lat, num_lon) for a single precipitation value.\n",
    "\n",
    "# A Conv2D layer can project the `filters` down to 1 channel (tp).\n",
    "# The input to this Conv2D will be (batch_size, num_lat, num_lon, last_convlstm_filters)\n",
    "# The output will be (batch_size, num_lat, num_lon, 1)\n",
    "# Then squeeze the last dimension.\n",
    "\n",
    "# An alternative for predicting a single 2D grid:\n",
    "# Using Conv3D to predict a single frame (n_steps_out = 1).\n",
    "# The output of the last ConvLSTM2D is (batch_size, num_lat, num_lon, filters).\n",
    "# We need to add a dimension for time step 1 to use Conv3D, or simply use Conv2D.\n",
    "# Let's use Conv2D to output the single frame prediction.\n",
    "\n",
    "# Conv2D output layer: 1 filter (for 'tp'), 1x1 kernel (to combine feature maps), linear activation.\n",
    "# This projects the feature maps from the last ConvLSTM2D into a single precipitation map.\n",
    "# If you want to predict n_steps_out > 1, you'd wrap this in a TimeDistributed layer\n",
    "# and use Conv3D with 1 filter for the final step.\n",
    "# For n_steps_out = 1, a simple Conv2D is appropriate to output the 2D grid.\n",
    "model.add(tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), activation='linear', padding='same')) # Outputting a 2D grid (lat, lon)\n",
    "\n",
    "# Squeeze the last dimension if it's 1 (from (batch, lat, lon, 1) to (batch, lat, lon))\n",
    "# Keras models automatically handle this if the loss function expects a different shape,\n",
    "# but it's good practice to ensure the model's output matches the target's shape.\n",
    "# We'll use a Lambda layer for this if n_steps_out is 1.\n",
    "if n_steps_out == 1:\n",
    "    # Ensure the model's output directly matches y_train_target's shape (batch, lat, lon)\n",
    "    model.add(tf.keras.layers.Lambda(lambda x: tf.squeeze(x, axis=-1)))\n",
    "\n",
    "\n",
    "# --- Compile the Model ---\n",
    "optimizer = Adam(learning_rate=0.001) # You can adjust learning rate\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae']) # Mean Absolute Error (MAE) is good for interpretability\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- Define Callbacks for Training ---\n",
    "# EarlyStopping: Stop training if validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# ModelCheckpoint: Save the best model based on validation loss\n",
    "model_checkpoint = ModelCheckpoint('best_convlstm_model.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   save_best_only=True,\n",
    "                                   mode='min',\n",
    "                                   verbose=1)\n",
    "\n",
    "# --- Train the Model ---\n",
    "print(\"\\nStarting model training...\")\n",
    "history = model.fit(X_train_normalized, y_train_target,\n",
    "                    epochs=50, # Number of epochs, adjust as needed\n",
    "                    batch_size=8, # Batch size, adjust based on GPU memory\n",
    "                    validation_split=0.2, # Use a portion of training data for validation\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "print(\"\\nModel training complete.\")\n",
    "\n",
    "# --- Evaluate the Model on the Test Set ---\n",
    "print(\"\\nEvaluating model on the test set...\")\n",
    "test_loss, test_mae = model.evaluate(X_test_normalized, y_test_target, verbose=1)\n",
    "print(f\"Test Loss (MSE): {test_loss:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# You can now use the trained `model` to make predictions:\n",
    "# predictions_normalized = model.predict(X_test_normalized)\n",
    "# Then inverse transform `predictions_normalized` using `scaler_y` to get original precipitation values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eb2d26-4a00-4637-92ef-3e9fd0c870da",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
